{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MHvuWIUHdIpt"
      },
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digit Recognition\n",
        "- Author = Amitrajit Bose\n",
        "- Dataset = MNIST\n",
        "- [Medium Article Link](https://medium.com/@amitrajit_bose/handwritten-digit-mnist-pytorch-977b5338e627)\n",
        "- Frameworks = PyTorch\n"
      ]
    },
    {
      "metadata": {
        "id": "oGjRmijsaXJ3"
      },
      "cell_type": "markdown",
      "source": [
        "### Necessary Imports"
      ]
    },
    {
      "metadata": {
        "id": "TOyGrPT5ASDc"
      },
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPuuTDEfAfDy"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uLdtrS4zaeEs"
      },
      "cell_type": "markdown",
      "source": [
        "### Download The Dataset & Define The Transforms"
      ]
    },
    {
      "metadata": {
        "id": "sZD2NGz2Ak6w"
      },
      "cell_type": "code",
      "source": [
        "### Run this cell\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "valset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GcAfrn2falkK"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploring The Data"
      ]
    },
    {
      "metadata": {
        "id": "xOjlOyjcCezX"
      },
      "cell_type": "code",
      "source": [
        "# Example corrected approach to iterate through trainloader\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)  # Get the next batch using next()\n",
        "\n",
        "print(type(images))  # Print type of images\n",
        "print(images.shape)  # Print shape of images tensor\n",
        "print(labels.shape)  # Print shape of labels tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EuBvOWmGDHOq"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9CppCcqDLtB"
      },
      "cell_type": "code",
      "source": [
        "figure = plt.figure()\n",
        "num_of_images = 60\n",
        "for index in range(1, num_of_images + 1):\n",
        "    plt.subplot(6, 10, index)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGyau0mOaP2m"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining The Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "n-NR96UtFSkB"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mlp_mnist.png)"
      ]
    },
    {
      "metadata": {
        "id": "3WJXInzQGcAy"
      },
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Layer details for the neural network\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyXEUFICQeqF"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxSLypv2LOD-"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "logps = model(images.cuda())\n",
        "loss = criterion(logps, labels.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pj4I2lLgLVWw"
      },
      "cell_type": "code",
      "source": [
        "print('Before backward pass: \\n', model[0].weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('After backward pass: \\n', model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0ZVHVbvI_yt"
      },
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dS9JqXhhLdkr"
      },
      "cell_type": "code",
      "source": [
        "print('Initial weights - ', model[0].weight)\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(64, 784)\n",
        "\n",
        "# Clear the gradients, do this because gradients are accumulated\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass, then backward pass, then update weights\n",
        "output = model(images.cuda())\n",
        "loss = criterion(output, labels.cuda())\n",
        "loss.backward()\n",
        "print('Gradient -', model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wy0KOQ95LgYN"
      },
      "cell_type": "code",
      "source": [
        "# Take an update step and few the new weights\n",
        "optimizer.step()\n",
        "print('Updated weights - ', model[0].weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wstRGu4FaJBe"
      },
      "cell_type": "markdown",
      "source": [
        "### Core Training Of Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "XCsoAdjdLjPb"
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "time0 = time()\n",
        "epochs = 15\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(images.cuda())\n",
        "        loss = criterion(output, labels.cuda())\n",
        "\n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "\n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75j9X1b6ME5K"
      },
      "cell_type": "code",
      "source": [
        "def view_classify(img, ps):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.cpu().data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels(np.arange(10))\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ie9Fffl_Mqp6"
      },
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(valloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = model(img.cuda())\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "probab = list(ps.cpu().numpy()[0])\n",
        "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAEvDtiaM6RQ"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "5sBPmaBONPkT"
      },
      "cell_type": "code",
      "source": [
        "correct_count, all_count = 0, 0\n",
        "for images,labels in valloader:\n",
        "  for i in range(len(labels)):\n",
        "    img = images[i].view(1, 784)\n",
        "    # Turn off gradients to speed up this part\n",
        "    with torch.no_grad():\n",
        "        logps = model(img.cuda())\n",
        "\n",
        "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "    ps = torch.exp(logps)\n",
        "    probab = list(ps.cpu().numpy()[0])\n",
        "    pred_label = probab.index(max(probab))\n",
        "    true_label = labels.numpy()[i]\n",
        "    if(true_label == pred_label):\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "\n",
        "print(\"Number Of Images Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}